# æœ¬åœ° AI æ™ºèƒ½æ–‡çŒ®ä¸å›¾åƒç®¡ç†åŠ©æ‰‹ï¼ˆLocal Multimodal AI Agentï¼‰

ä¸€ä¸ª**çº¯æœ¬åœ°è¿è¡Œ**çš„å¤šæ¨¡æ€çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œç”¨äºç®¡ç† PDF æ–‡çŒ®ä¸å›¾ç‰‡ç´ æã€‚  
ç›¸æ¯”â€œæŒ‰æ–‡ä»¶åæœç´¢â€ï¼Œæœ¬é¡¹ç›®é€šè¿‡**è¯­ä¹‰å‘é‡æ£€ç´¢**å®ç°å¯¹è®ºæ–‡å†…å®¹ä¸å›¾ç‰‡è¯­ä¹‰çš„å¿«é€Ÿå®šä½ï¼Œå¹¶æ”¯æŒè‡ªåŠ¨å½’æ¡£ä¸ç´¢å¼•ç»´æŠ¤ã€‚
- å§“åï¼š æåŒè½©
- å­¦å·ï¼š 25120324
---

## åŠŸèƒ½æ¦‚è§ˆ

### 1) æ™ºèƒ½æ–‡çŒ®ç®¡ç†ï¼ˆPDFï¼‰
- **è¯­ä¹‰æœç´¢ï¼ˆpaper-level èšåˆï¼‰**ï¼šè¾“å…¥è‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„è®ºæ–‡ï¼Œå¹¶åœ¨æ¯ç¯‡è®ºæ–‡ä¸‹å±•ç¤ºå¤šä¸ªç›¸å…³ç‰‡æ®µï¼ˆé¿å…å•ç¯‡è®ºæ–‡å æ»¡å€™é€‰ï¼‰ã€‚
- **è‡ªåŠ¨åˆ†ç±»æ•´ç†**
  - **å•æ–‡ä»¶**ï¼šæ·»åŠ è®ºæ–‡æ—¶æŒ‰ `--topics` è‡ªåŠ¨å½’ç±»åˆ° `library/<topic>/`
  - **æ‰¹é‡**ï¼šå¯¹æ–‡ä»¶å¤¹å†… PDF é€’å½’æ•´ç†å¹¶å»ºç«‹ç´¢å¼•
- **ç´¢å¼•ç»´æŠ¤**
  - **stats**ï¼šæŸ¥çœ‹å½“å‰ç´¢å¼•çŠ¶æ€ï¼ˆè®ºæ–‡ chunk æ•°ã€å›¾ç‰‡æ•°ã€ç›®å½•ä¸æ•°æ®åº“è·¯å¾„ï¼‰
  - **rebuild_index**ï¼šä» `library/` ä¸ `datasets/images/` å…¨é‡é‡å»ºç´¢å¼•ï¼Œä¿è¯ä¸€è‡´æ€§
  - **remove_paper**ï¼šä»ç´¢å¼•ä¸­ç§»é™¤æŒ‡å®šè®ºæ–‡

### 2) æ™ºèƒ½å›¾åƒç®¡ç†ï¼ˆImagesï¼‰
- **ä»¥æ–‡æœå›¾ï¼ˆtext-to-imageï¼‰**ï¼šè¾“å…¥æ–‡æœ¬æè¿°ï¼ˆå¦‚ â€œsunset by the seaâ€ï¼‰æ£€ç´¢æœ¬åœ°å›¾ç‰‡åº“ï¼Œè¿”å›æœ€åŒ¹é…çš„å›¾ç‰‡è·¯å¾„ã€‚

---

## é¡¹ç›®ç»“æ„
> æˆ‘ç”¨çš„ä¿©æ¨¡å‹å¤ªå¤§äº†ï¼Œä¼ ä¸ä¸Šæ¥ï¼Œhuggingfaceä¸Šä¸‹è½½ä¸‹æ¥æ”¾åœ¨æ–‡ä»¶å¤¹ä¸‹ï¼ˆæˆ‘çš„åšæ³•ï¼‰æˆ–è€…ç¼“å­˜éƒ½è¡Œï¼Œåœ¨config.pyä¸­ä¿®æ”¹æ¨¡å‹è·¯å¾„å³å¯ï¼ˆå« json çš„é‚£ä¸ªæ–‡ä»¶å¤¹ï¼‰ã€‚

```text
.
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ papers/          # åŸå§‹ PDF è¾“å…¥
â”‚   â””â”€â”€ images/          # åŸå§‹å›¾ç‰‡è¾“å…¥
â”œâ”€â”€ library/             # å½’æ¡£åçš„è®ºæ–‡ç›®å½•ï¼ˆæŒ‰ topic åˆ†æ–‡ä»¶å¤¹ï¼‰
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ chroma_papers/   # è®ºæ–‡å‘é‡åº“ï¼ˆChromaDBï¼‰
â”‚   â””â”€â”€ chroma_images/   # å›¾ç‰‡å‘é‡åº“ï¼ˆChromaDBï¼‰
â”œâ”€â”€ main.py              # CLI ç»Ÿä¸€å…¥å£
â”œâ”€â”€ config.py            # è·¯å¾„ä¸è¶…å‚é…ç½®
â””â”€â”€ ...
```

---

## ç¯å¢ƒä¸ä¾èµ–

- **Miniforge Python 3.10**
- **æœ¬åœ°æ¨¡å‹ï¼ˆä»“åº“å†…æœ¬åœ°åŠ è½½, æ›´æ¢ç”µè„‘åœ¨config.pyéœ€è¦ä¸­æ›´æ”¹æ¨¡å‹åœ°å€ï¼‰**ï¼š
  - **æ–‡æœ¬åµŒå…¥**ï¼šall-MiniLM-L6-v2
  - **å›¾æ–‡åŒ¹é…**ï¼šCLIP ViT-B-32
---

## å®‰è£…ä¾èµ–ï¼š

`pip install -r requirements.txt`



---

## å¿«é€Ÿå¼€å§‹ï¼ˆå‘½ä»¤ç¤ºä¾‹ï¼‰

- **ç”±äºæ¨¡å‹èƒ½åŠ›çš„é™åˆ¶ï¼Œä½¿ç”¨æ±‰è¯­è¿›è¡Œæœç´¢çš„æ—¶å€™å¯èƒ½ä¼šå‡ºç°é©´å”‡ä¸å¯¹é©¬å˜´çš„æƒ…å†µï¼Œè¿™ä¸æ˜¯ä»£ç  bugï¼å½“å‰ç”¨çš„æ˜¯ OpenAI åŸç‰ˆ CLIP ViT-B/32ï¼Œä¸»è¦åœ¨è‹±æ–‡æ–‡æœ¬ä¸Šè®­ç»ƒï¼Œä¸­æ–‡æè¿°çš„è¯­ä¹‰å¯¹é½è¾ƒå¼±ï¼Œæ‰€ä»¥ä¾‹å¦‚æ£€ç´¢å›¾åƒæ—¶ä½¿ç”¨â€œæ ‘â€ï¼Œä½†â€œcampus treeâ€è¢«æ’åˆ°æœ€åä¸€ä½ã€‚ä»£ç é‡Œæ£€ç´¢ç”¨çš„æ˜¯æ ‡å‡†çš„å½’ä¸€åŒ– CLIP ç‰¹å¾ï¼Œé€»è¾‘æ˜¯æ­£ç¡®çš„ã€‚å¼ºçƒˆå»ºè®®ä½¿ç”¨è‹±æ–‡è¿›è¡Œæœç´¢ï¼Œä»¥ä¿è¯é¡¹ç›®çš„å¯å¤ç°ï¼ï¼ï¼**

- **æ‰€æœ‰åŠŸèƒ½é€šè¿‡ main.py è°ƒç”¨ï¼š**


### æŸ¥çœ‹å¸®åŠ©
`python main.py --help`

### 1) æ·»åŠ å¹¶æŒ‰ä¸»é¢˜åˆ†ç±»å•ç¯‡è®ºæ–‡ï¼ˆå¤åˆ¶åˆ° library/<topic>/ å¹¶å»ºç«‹ç´¢å¼•ï¼‰
`python main.py add_paper datasets/papers/BERT.pdf --topics "CV,NLP,RL"`

### 2) æ‰¹é‡æ•´ç†/ç´¢å¼•æ•´ä¸ªæ–‡ä»¶å¤¹ï¼ˆé€’å½’å¤„ç† PDFï¼‰
`python main.py organize datasets/papers --topics "CV,NLP,RL"`

### 3) è¯­ä¹‰æœç´¢è®ºæ–‡ï¼ˆç´¢å¼•ä¸ºç©ºæ—¶ä¼šè‡ªåŠ¨ä» library/ å»ºç´¢å¼•ï¼‰
`python main.py search_paper "Use cases of Transformer." --top_k 7`

### 4) ä»¥æ–‡æœå›¾ï¼ˆç´¢å¼•ä¸ºç©ºæ—¶ä¼šè‡ªåŠ¨ç´¢å¼• datasets/imagesï¼‰
`python main.py search_image "sunset" --top_k 3`

### 5) æŸ¥çœ‹ç´¢å¼•çŠ¶æ€ï¼ˆè®ºæ–‡ chunk æ•°ã€å›¾ç‰‡æ•°ã€è·¯å¾„ç­‰ï¼‰
`python main.py stats`

### 6) å…¨é‡é‡å»ºç´¢å¼•ï¼ˆå½“æ›´æ¢æ¨¡å‹ã€chunk_sizeã€æˆ–ç§»åŠ¨/åˆ é™¤è®ºæ–‡åå»ºè®®æ‰§è¡Œï¼‰
`python main.py rebuild_index`


---



## å…³é”®è®¾è®¡è¯´æ˜ï¼ˆWhy it worksï¼‰

### 1) PDF åˆ†å—ä¸æ£€ç´¢å•å…ƒ

- ä½¿ç”¨ `pypdf` æå–æ–‡æœ¬  
- æŒ‰ `config.PDF_CHUNK_SIZE` åˆ†å—ï¼ˆchunkï¼‰  
- æ¯ä¸ª chunk å»ºç«‹å‘é‡å¹¶å†™å…¥ ChromaDB  
- æ£€ç´¢æ—¶ä»¥ chunk ä¸ºåŸºç¡€å¬å›å€™é€‰ï¼Œå†åš paper-level èšåˆè¾“å‡ºï¼ˆä¸€ç¯‡è®ºæ–‡æœ€å¤šå±•ç¤º N ä¸ªç‰‡æ®µï¼‰

> è¿™æ ·åšæ—¢èƒ½å‘½ä¸­å…·ä½“å†…å®¹ï¼Œåˆèƒ½é¿å…â€œåŒä¸€ç¯‡è®ºæ–‡å æ»¡ top_kâ€çš„é—®é¢˜ã€‚

### 2) æ–‡æœ¬ä¸å›¾ç‰‡å‘é‡

- **æ–‡æœ¬**ï¼šSentenceTransformersï¼ˆ`all-MiniLM-L6-v2`ï¼‰ï¼Œå½’ä¸€åŒ–å‘é‡  
- **å›¾ç‰‡**ï¼šCLIP image encoderï¼›æ£€ç´¢æ—¶ç”¨ CLIP text encoder ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œä¸å›¾ç‰‡å‘é‡å¯¹é½

### 3) ç´¢å¼•ä¸€è‡´æ€§ï¼ˆ`stats` / `rebuild_index`ï¼‰

- `stats` æä¾›å¯è§‚å¯Ÿæ€§ï¼Œä¾¿äºè°ƒè¯•ä¸æ¼”ç¤º  
- `rebuild_index` ç”¨äºæ¨¡å‹å‡çº§ã€å‚æ•°å˜æ›´ã€æˆ–åº“å†…å®¹å˜åŠ¨åçš„å…¨é‡é‡å»ºï¼Œé¿å…â€œæ–°æ—§ embedding æ··ç”¨â€å¯¼è‡´æ£€ç´¢å¼‚å¸¸

---

## è¿è¡Œç»“æœï¼ˆèŠ‚é€‰ï¼‰

###  1) æ‰¹é‡æ•´ç†ä¸åˆ†ç±»

```bash
> python main.py organize datasets/papers --topics "CV,NLP,RL"
2025-12-17 16:38:45,056 [INFO] Loading text model from /Users/tonglion/PycharmProjects/Experiment2/all-MiniLM-L6-v2
2025-12-17 16:38:45,057 [INFO] Load pretrained SentenceTransformer: /Users/tonglion/PycharmProjects/Experiment2/all-MiniLM-L6-v2
2025-12-17 16:38:45,143 [INFO] Loading CLIP model from /Users/tonglion/PycharmProjects/Experiment2/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.'
2025-12-17 16:38:45,773 [INFO] Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-12-17 16:38:45,896 [INFO] Connected to Chroma collection=papers at /Users/tonglion/PycharmProjects/Experiment2/storage/chroma_papers
2025-12-17 16:38:45,897 [INFO] Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-12-17 16:38:45,905 [INFO] Connected to Chroma collection=images at /Users/tonglion/PycharmProjects/Experiment2/storage/chroma_images
Organizing papers:   0%|                                                                                                                    | 0/10 [00:00<?, ?it/s]2025-12-17 16:38:45,923 [INFO] Copied into library /Users/tonglion/PycharmProjects/Experiment2/datasets/papers/Ï€  3  - Permutation-Equivariant Visual Geometry Learning.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/Ï€  3  - Permutation-Equivariant Visual Geometry Learning.pdf
2025-12-17 16:38:47,157 [INFO] Classified /Users/tonglion/PycharmProjects/Experiment2/library/Ï€  3  - Permutation-Equivariant Visual Geometry Learning.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/CV/Ï€  3  - Permutation-Equivariant Visual Geometry Learning.pdf (score=0.342)
Organizing papers:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                 | 1/10 [00:01<00:15,  1.71s/it]2025-12-17 16:38:47,624 [INFO] Copied into library /Users/tonglion/PycharmProjects/Experiment2/datasets/papers/Proximal Policy Optimization Algorithms.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/Proximal Policy Optimization Algorithms.pdf
2025-12-17 16:38:51,122 [INFO] Classified /Users/tonglion/PycharmProjects/Experiment2/library/Proximal Policy Optimization Algorithms.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/RL/Proximal Policy Optimization Algorithms.pdf (score=0.353)
Organizing papers:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                      | 2/10 [00:05<00:23,  2.90s/it]2025-12-17 16:38:51,361 [INFO] Copied into library /Users/tonglion/PycharmProjects/Experiment2/datasets/papers/Soft Adaptive Policy Optimization.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/Soft Adaptive Policy Optimization.pdf
2025-12-17 16:38:52,069 [INFO] Classified /Users/tonglion/PycharmProjects/Experiment2/library/Soft Adaptive Policy Optimization.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/RL/Soft Adaptive Policy Optimization.pdf (score=0.317)
Organizing papers:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                           | 3/10 [00:06<00:14,  2.01s/it]2025-12-17 16:38:52,319 [INFO] Copied into library /Users/tonglion/PycharmProjects/Experiment2/datasets/papers/Gemini- A Family of Highly Capable  Multimodal Models.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/Gemini- A Family of Highly Capable  Multimodal Models.pdf
2025-12-17 16:38:54,391 [INFO] Classified /Users/tonglion/PycharmProjects/Experiment2/library/Gemini- A Family of Highly Capable  Multimodal Models.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/CV/Gemini- A Family of Highly Capable  Multimodal Models.pdf (score=0.358)
Organizing papers:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                | 4/10 [00:10<00:15,  2.66s/it]2025-12-17 16:38:55,978 [INFO] Copied into library /Users/tonglion/PycharmProjects/Experiment2/datasets/papers/Mamba- Linear-Time Sequence Modeling with Selective State Spaces.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/Mamba- Linear-Time Sequence Modeling with Selective State Spaces.pdf
2025-12-17 16:38:57,097 [INFO] Classified /Users/tonglion/PycharmProjects/Experiment2/library/Mamba- Linear-Time Sequence Modeling with Selective State Spaces.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/NLP/Mamba- Linear-Time Sequence Modeling with Selective State Spaces.pdf (score=0.222)
Organizing papers:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                      | 5/10 [00:12<00:12,  2.48s/it]2025-12-17 16:38:58,136 [INFO] Copied into library /Users/tonglion/PycharmProjects/Experiment2/datasets/papers/BERT.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/BERT.pdf
2025-12-17 16:38:58,914 [INFO] Classified /Users/tonglion/PycharmProjects/Experiment2/library/BERT.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/NLP/BERT.pdf (score=0.471)
Organizing papers:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                           | 6/10 [00:13<00:08,  2.08s/it]2025-12-17 16:38:59,430 [INFO] Copied into library /Users/tonglion/PycharmProjects/Experiment2/datasets/papers/DeepSeekMath- Pushing the Limits of Mathematical Reasoning in Open Language Models.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/DeepSeekMath- Pushing the Limits of Mathematical Reasoning in Open Language Models.pdf
2025-12-17 16:39:00,037 [INFO] Classified /Users/tonglion/PycharmProjects/Experiment2/library/DeepSeekMath- Pushing the Limits of Mathematical Reasoning in Open Language Models.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/NLP/DeepSeekMath- Pushing the Limits of Mathematical Reasoning in Open Language Models.pdf (score=0.348)
Organizing papers:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 7/10 [00:14<00:05,  1.80s/it]2025-12-17 16:39:00,659 [INFO] Copied into library /Users/tonglion/PycharmProjects/Experiment2/datasets/papers/GPT-4o System Card.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/GPT-4o System Card.pdf
2025-12-17 16:39:01,186 [INFO] Classified /Users/tonglion/PycharmProjects/Experiment2/library/GPT-4o System Card.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/CV/GPT-4o System Card.pdf (score=0.263)
Organizing papers:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 8/10 [00:15<00:03,  1.58s/it]2025-12-17 16:39:01,767 [INFO] Copied into library /Users/tonglion/PycharmProjects/Experiment2/datasets/papers/VGGT- Visual Geometry Grounded Transformer.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/VGGT- Visual Geometry Grounded Transformer.pdf
2025-12-17 16:39:02,591 [INFO] Classified /Users/tonglion/PycharmProjects/Experiment2/library/VGGT- Visual Geometry Grounded Transformer.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/CV/VGGT- Visual Geometry Grounded Transformer.pdf (score=0.357)
Organizing papers:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 9/10 [00:17<00:01,  1.57s/it]2025-12-17 16:39:03,310 [INFO] Copied into library /Users/tonglion/PycharmProjects/Experiment2/datasets/papers/Attention Is All You Need.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/Attention Is All You Need.pdf
2025-12-17 16:39:04,013 [INFO] Classified /Users/tonglion/PycharmProjects/Experiment2/library/Attention Is All You Need.pdf -> /Users/tonglion/PycharmProjects/Experiment2/library/NLP/Attention Is All You Need.pdf (score=0.288)
Organizing papers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.84s/it]
2025-12-17 16:39:04,354 [INFO] Organized 10 papers

```
###  2) è¯­ä¹‰æœç´¢ï¼ˆpaper-level èšåˆç»“æœï¼‰

```bash
> python main.py search_paper "Use cases of Transformer." --top_k 7
[1] /Users/tonglion/PycharmProjects/Experiment2/library/NLP/Mamba- Linear-Time Sequence Modeling with Selective State Spaces.pdf (topic=NLP, score=-0.369)
  (1) [chunk 43] score=-0.369: Donald Metzler. â€œLong Range Arena: A Benchmark for Efficient Transformersâ€. In: International Conference on Learning Representations (ICLR) . 2021. [104] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. â€œEfficie...
  (2) [chunk 29] score=-0.427: model, Mamba can achieve 5Ã—higher throughput than Transformers. 4.6 Model Ablations We perform a series of detailed ablations on components of our model, focusing on the setting of language modeling with size â‰ˆ350M model...
------------------------------------------------------------
[2] /Users/tonglion/PycharmProjects/Experiment2/library/NLP/Attention Is All You Need.pdf (topic=NLP, score=-0.377)
  (1) [chunk 11] score=-0.377: 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. Model BLEU Training Cost (FLOPs) ...
  (2) [chunk 6] score=-0.528: h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full d...
------------------------------------------------------------
[3] /Users/tonglion/PycharmProjects/Experiment2/library/NLP/BERT.pdf (topic=NLP, score=-0.436)
  (1) [chunk 5] score=-0.436: and ï¬ne-tuning. Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks. For ï¬ne- tuning, the BERT model is ï¬rst initialized with the pre-trained parameters, and all of the param- ...
  (2) [chunk 0] score=-0.671: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout}@google.com Abstrac...
------------------------------------------------------------
[4] /Users/tonglion/PycharmProjects/Experiment2/library/CV/VGGT- Visual Geometry Grounded Transformer.pdf (topic=CV, score=-0.509)
  (1) [chunk 46] score=-0.509: . Vasilakos, and Thippa Reddy Gadekallu. Generative pre-trained trans- former: A comprehensive review on enabling technologies, potential applications, emerging challenges, and future di- rections. arXiv.cs, abs/2305.104...
  (2) [chunk 35] score=-0.553: Vision . Cambridge University Press, ISBN: 0521540518, 2004. 13 [47] Xingyi He, Jiaming Sun, Yifan Wang, Sida Peng, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Detector-free struc- ture from motion. In arxiv, 2023. 12 [48...
------------------------------------------------------------
[5] /Users/tonglion/PycharmProjects/Experiment2/library/CV/Gemini- A Family of Highly Capable  Multimodal Models.pdf (topic=CV, score=-0.653)
  (1) [chunk 80] score=-0.653: of Highly Capable Multimodal Models 9. Contributions and Acknowledgments Gemini Leads Rohan Anil,Co-Lead, Text Sebastian Borgeaud,Co-Lead, Text Jean-Baptiste Alayrac,Co-Lead, MM Vision Jiahui Yu,Co-Lead, MM Vision Radu S...
  (2) [chunk 58] score=-0.705: feedback across safety and other domain areas through the user interface, and where possible, in-depth interviews. Focus areas included safety and persona, functionality, coding and instruction capabilities, and factuali...
------------------------------------------------------------
[6] /Users/tonglion/PycharmProjects/Experiment2/library/CV/Ï€  3  - Permutation-Equivariant Visual Geometry Learning.pdf (topic=CV, score=-0.716)
  (1) [chunk 26] score=-0.716: this design can introduce noticeable grid-like artifacts, particularly in regions with high reconstruction uncertainty. 14...
  (2) [chunk 23] score=-0.757: whereas VGGT uses 48. The decoders for camera poses, local point maps, and confidence scores share the same architecture but do not share weights. This architecture is a lightweight, 5-layer transformer that applies self...
------------------------------------------------------------
[7] /Users/tonglion/PycharmProjects/Experiment2/library/CV/GPT-4o System Card.pdf (topic=CV, score=-0.765)
  (1) [chunk 28] score=-0.765: Zhong, Mia Glaese, Nick Turley, Noah Deutsch, Noel Bundick, Ola Okelola, Olivier Godement, Owen Campbell-Moore, Peter Bak, Peter Bakkum, Raul Puri, Rowan Zellers, Saachi Jain, Shantanu Jain, Shirong Wu, Spencer Papay, Ta...
------------------------------------------------------------

> python main.py search_paper "visual" --top_k 7
[1] /Users/tonglion/PycharmProjects/Experiment2/library/CV/VGGT- Visual Geometry Grounded Transformer.pdf (topic=CV, score=-0.135)
  (1) [chunk 0] score=-0.135: VGGT: Visual Geometry Grounded Transformer Jianyuan Wang1,2 Minghao Chen1,2 Nikita Karaev1,2 Andrea Vedaldi1,2 Christian Rupprecht1 David Novotny2 1Visual Geometry Group, University of Oxford 2Meta AI â€¦ Figure 1. VGGT is...
  (2) [chunk 19] score=-0.185: Input Images Ground Truth Prediction Figure 6. Qualitative Examples of Novel View Synthesis. The top row shows the input images, the middle row displays the ground truth images from target viewpoints, and the bottom row ...
------------------------------------------------------------
[2] /Users/tonglion/PycharmProjects/Experiment2/library/CV/Gemini- A Family of Highly Capable  Multimodal Models.pdf (topic=CV, score=-0.150)
  (1) [chunk 70] score=-0.150: Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai- Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.arXiv preprint ar...
  (2) [chunk 98] score=-0.196: This section shows sample qualitative examples from prompting the Gemini Ultra model. Some illustrative examples of multimodal reasoning for image understanding tasks over charts, natural images and memes are shown in Fi...
------------------------------------------------------------
[3] /Users/tonglion/PycharmProjects/Experiment2/library/CV/GPT-4o System Card.pdf (topic=CV, score=-0.195)
  (1) [chunk 1] score=-0.195: text and vision capabilities of GPT-4o, depending on the risk assessed. This is indicated accordingly throughout the System Card. 1 arXiv:2410.21276v1 [cs.CL] 25 Oct 2024 â€¢ Proprietary data from data partnerships.We form...
  (2) [chunk 27] score=-0.290: Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Christina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongji...
------------------------------------------------------------
[4] /Users/tonglion/PycharmProjects/Experiment2/library/CV/Ï€  3  - Permutation-Equivariant Visual Geometry Learning.pdf (topic=CV, score=-0.223)
  (1) [chunk 0] score=-0.223: Ï€3: Permutation-Equivariant Visual Geometry Learning Yifan Wang1âˆ— Jianjun Zhou123âˆ— Haoyi Zhu1 Wenzheng Chang1 Yang Zhou1 Zizun Li1 Junyi Chen1 Jiangmiao Pang1 Chunhua Shen2 Tong He13â€  1Shanghai AI Lab 2ZJU 3SII âˆ—Equal Co...
  (2) [chunk 21] score=-0.294: Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer.arXiv preprint arXiv:2503.11651, 2025. [35] Kaixuan Wang and Shaojie Shen. Fl...
------------------------------------------------------------
[5] /Users/tonglion/PycharmProjects/Experiment2/library/NLP/Attention Is All You Need.pdf (topic=NLP, score=-0.391)
  (1) [chunk 4] score=-0.391: of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent pos...
------------------------------------------------------------
[6] /Users/tonglion/PycharmProjects/Experiment2/library/NLP/BERT.pdf (topic=NLP, score=-0.406)
  (1) [chunk 25] score=-0.406: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances...
------------------------------------------------------------

> python main.py search_paper "Use cases of reinforcement learning." --top_k 7
[1] /Users/tonglion/PycharmProjects/Experiment2/library/NLP/DeepSeekMath- Pushing the Limits of Mathematical Reasoning in Open Language Models.pdf (topic=NLP, score=-0.121)
  (1) [chunk 29] score=-0.121: techniques (Kwon et al., 2023; Leviathan et al., 2023; Xia et al., 2023, 2024), which determines 21 the exploration efficiency of policy models, also play an exceedingly important role. Algorithms Algorithms process the ...
  (2) [chunk 19] score=-0.123: of each reasoning step. Formally, given the question ğ‘and ğº sampled outputs {ğ‘œ1, ğ‘œ2, Â·Â·Â· , ğ‘œğº}, a process reward model is used to score each step of the outputs, yielding corresponding rewards: R = {{ğ‘Ÿğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥(1) 1 , Â·Â·Â· , ...
------------------------------------------------------------
[2] /Users/tonglion/PycharmProjects/Experiment2/library/RL/Proximal Policy Optimization Algorithms.pdf (topic=RL, score=-0.167)
  (1) [chunk 11] score=-0.167: J. Schulman, J. Tang, and W. Zaremba. â€œOpenAI Gymâ€. In: arXiv preprint arXiv:1606.01540 (2016). [Dua+16] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. â€œBenchmarking Deep Reinforcement Learning for Continuou...
  (2) [chunk 0] score=-0.217: Proximal Policy Optimization Algorithms John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov OpenAI {joschu, filip, prafulla, alec, oleg}@openai.com Abstract We propose a new family of policy gradien...
------------------------------------------------------------
[3] /Users/tonglion/PycharmProjects/Experiment2/library/NLP/Mamba- Linear-Time Sequence Modeling with Selective State Spaces.pdf (topic=NLP, score=-0.242)
  (1) [chunk 16] score=-0.242: (and general LTI models). On the other hand, selective models can simply reset their state at any time to remove extraneous history, and thus their performance in principle improves monotonicly with context length (e.g. ...
  (2) [chunk 37] score=-0.270: Models with Generalized Basis Projectionsâ€. In: The International Conference on Learning Representations (ICLR) . 2023. [42] Ankit Gupta, Albert Gu, and Jonathan Berant. â€œDiagonal State Spaces are as Effective as Structu...
------------------------------------------------------------
[4] /Users/tonglion/PycharmProjects/Experiment2/library/CV/Gemini- A Family of Highly Capable  Multimodal Models.pdf (topic=CV, score=-0.333)
  (1) [chunk 33] score=-0.333: measures human preference in domains such as travel planning and video discovery. We find models equipped with tools are preferred on this set 78% of the time over models without tools (excluding ties). Gemini API models...
  (2) [chunk 30] score=-0.336: to our models provides further gains over SFT alone. Our approach creates an iterative process in which RL continually pushes the boundaries of the RM, while the RM is continuously improved through evaluation and data co...
------------------------------------------------------------
[5] /Users/tonglion/PycharmProjects/Experiment2/library/RL/Soft Adaptive Policy Optimization.pdf (topic=RL, score=-0.371)
  (1) [chunk 6] score=-0.371: ) bAi,t âˆ‚zv = âˆ‚Ï€Î¸(yi,t |q,y i,<t ) âˆ‚zv Â· bAi,t Ï€Î¸(yi,t |q,y i,<t ) = 1(v=y i,t )exp(z yi,t ) âˆ‘vâ€²âˆˆV exp(zvâ€² )âˆ’exp(z yi,t )exp(z v) (âˆ‘vâ€²âˆˆV exp(zvâ€² ))2 Â· bAi,t Ï€Î¸(yi,t |q,y i,<t ) = ( 1âˆ’Ï€ Î¸(yi,t |q,y i,<t )  Â· bAi,t ifv=y...
  (2) [chunk 14] score=-0.394: on Artificial Intelligence, volume 37, pages 7078â€“7086, 2023. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.arXiv preprint arXiv:2501.12948, 2025. HMMT. Hmmt 2025.https:/...
------------------------------------------------------------
[6] /Users/tonglion/PycharmProjects/Experiment2/library/CV/GPT-4o System Card.pdf (topic=CV, score=-0.402)
  (1) [chunk 37] score=-0.402: â€œParadigm: Improving patient access to clinical trials.â€https://openai.com/index/paradigm/, 2024. Accessed: 2024-08-07. [49] M. Hutson, â€œHow ai is being used to accelerate clinical trials,â€Nature, vol. 627, pp. S2â€“S5, 20...
  (2) [chunk 18] score=-0.433: scheming. Capability Evaluation Description Performance Self-Knowledge "SAD" Benchmark (3 tasks) QA evaluations of a modelâ€™s knowledge of itself and how it can causally influence the rest of the world. â€¢â€¢â—¦ Explicit Theor...
------------------------------------------------------------
```
###  3) ç´¢å¼•çŠ¶æ€ä¸é‡å»º

```bash
> python main.py stats
Papers indexed chunks: 418
Images indexed: 0
Library dir: /Users/tonglion/PycharmProjects/Experiment2/library
Paper DB: /Users/tonglion/PycharmProjects/Experiment2/storage/chroma_papers
Image DB: /Users/tonglion/PycharmProjects/Experiment2/storage/chroma_images

> python main.py rebuild_index
2025-12-17 16:43:39,413 [INFO] Loading text model from /Users/tonglion/PycharmProjects/Experiment2/all-MiniLM-L6-v2
2025-12-17 16:43:39,414 [INFO] Load pretrained SentenceTransformer: /Users/tonglion/PycharmProjects/Experiment2/all-MiniLM-L6-v2
2025-12-17 16:43:39,498 [INFO] Loading CLIP model from /Users/tonglion/PycharmProjects/Experiment2/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.'
2025-12-17 16:43:40,088 [INFO] Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-12-17 16:43:40,173 [INFO] Connected to Chroma collection=papers at /Users/tonglion/PycharmProjects/Experiment2/storage/chroma_papers
2025-12-17 16:43:40,174 [INFO] Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-12-17 16:43:40,177 [INFO] Connected to Chroma collection=images at /Users/tonglion/PycharmProjects/Experiment2/storage/chroma_images
2025-12-17 16:43:40,177 [INFO] Rebuilding paper index from library /Users/tonglion/PycharmProjects/Experiment2/library
Indexing papers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.69s/it]
2025-12-17 16:43:57,172 [INFO] Rebuilding image index from /Users/tonglion/PycharmProjects/Experiment2/datasets/images
2025-12-17 16:43:57,465 [INFO] Indexed 4 images from /Users/tonglion/PycharmProjects/Experiment2/datasets/images
2025-12-17 16:43:57,465 [INFO] Done.

> python main.py stats
2025-12-17 16:44:07,745 [INFO] Loading text model from /Users/tonglion/PycharmProjects/Experiment2/all-MiniLM-L6-v2
2025-12-17 16:44:07,746 [INFO] Load pretrained SentenceTransformer: /Users/tonglion/PycharmProjects/Experiment2/all-MiniLM-L6-v2
2025-12-17 16:44:07,795 [INFO] Loading CLIP model from /Users/tonglion/PycharmProjects/Experiment2/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.'
2025-12-17 16:44:08,356 [INFO] Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-12-17 16:44:08,466 [INFO] Connected to Chroma collection=papers at /Users/tonglion/PycharmProjects/Experiment2/storage/chroma_papers
2025-12-17 16:44:08,466 [INFO] Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-12-17 16:44:08,469 [INFO] Connected to Chroma collection=images at /Users/tonglion/PycharmProjects/Experiment2/storage/chroma_images
Papers indexed chunks: 418
Images indexed: 4
Library dir: /Users/tonglion/PycharmProjects/Experiment2/library
Paper DB: /Users/tonglion/PycharmProjects/Experiment2/storage/chroma_papers
Image DB: /Users/tonglion/PycharmProjects/Experiment2/storage/chroma_images
```
###  4) ä»¥æ–‡æœå›¾

```bash
>  python main.py search_image "sunset" --top_k 3
[1] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/sunset by the sea.png (sunset by the sea.png)
[2] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/village field.png (village field.png)
[3] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/campus tree.png (campus tree.png)

>  python main.py search_image "perple" --top_k 3
[1] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/campus tree.png (campus tree.png)
[2] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/village field.png (village field.png)
[3] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/sunset by the sea.png (sunset by the sea.png)
  
                                                                                                                                                      
> python main.py search_image "people" --top_k 3
[1] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/street walker.png (street walker.png)
[2] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/sunset by the sea.png (sunset by the sea.png)
[3] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/village field.png (village field.png)

> python main.py search_image "ground" --top_k 3
[1] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/campus tree.png (campus tree.png)
[2] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/village field.png (village field.png)
[3] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/sunset by the sea.png (sunset by the sea.png)

> python main.py search_image "plants" --top_k 3
[1] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/campus tree.png (campus tree.png)
[2] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/village field.png (village field.png)
[3] /Users/tonglion/PycharmProjects/Experiment2/datasets/images/sunset by the sea.png (sunset by the sea.png)
```
###  5) æ–‡çŒ®åˆ†ç±»æ•ˆæœç¤ºä¾‹ï¼ˆCVï¼ŒNLPï¼ŒRLï¼‰
- åˆ†ç±»æ•ˆæœåå‘äºè®ºæ–‡è¯­è¨€æè¿°ï¼Œæ¯”å¦‚ deepseek çš„ DeepSeekMath- Pushing the Limits of Mathematical Reasoning in Open Language Modelsï¼Œæˆ‘è§‰å¾—å®ƒå±äº RL å’Œ NLP éƒ½è¡Œï¼Œæ¨¡å‹æŠŠå®ƒåˆ†åˆ°äº† NLPã€‚
  
<img width="97" height="100" alt="æˆªå±2025-12-17 ä¸‹åˆ7 09 26" src="https://github.com/user-attachments/assets/18b7c4b2-5bb0-40b6-a01d-c2a12a441fca" />
<img width="514" height="312" alt="æˆªå±2025-12-17 ä¸‹åˆ7 13 29" src="https://github.com/user-attachments/assets/aa21fbad-8a73-47df-9ac7-92d984c1e083" />

## å¯é€‰ä¼˜åŒ–æ–¹å‘ï¼ˆåç»­å¯æ‰©å±•ï¼‰
- å‚è€ƒæ–‡çŒ®/è‡´è°¢å™ªå£°è¿‡æ»¤ï¼šå‡å°‘å¼•ç”¨æ®µè½å¯¹æ£€ç´¢çš„å¹²æ‰°
- è¿”å›é¡µç /æ®µè½å®šä½ï¼šåœ¨å…ƒæ•°æ®ä¸­å­˜å‚¨ page/chunk â†’ é¡µç æ˜ å°„
- æ”¯æŒæ›´å¤šä¸»é¢˜ä¸å¤šæ ‡ç­¾åˆ†ç±»ï¼šä¸ä»…é€‰ 1 ä¸ª topicï¼Œå¯è¾“å‡º top-2/top-3
- GUI/REST APIï¼šFastAPI + ç®€æ˜“å‰ç«¯ï¼Œæå‡æ¼”ç¤ºæ•ˆæœ

